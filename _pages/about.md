---
title: "About"
permalink: /about/
author_profile: true
---

I am a PhD candidate in Computer Science and Engineering at the University of Notre Dame, where my research focuses on **security, robustness, and evaluation of large language models (LLMs)** and **privacy risks in networked immersive systems**.

My work lies at the intersection of **AI security, trustworthy machine learning, and applied systems research**. I study how modern learning systems fail under realistic adversarial and distributional conditions, and I design evaluation frameworks and defenses that improve reliability without requiring expensive retraining.

A major part of my research investigates **LLM robustness and safety**, including:
- Adversarial testing of LLMs under prompt injection and jailbreak attacks  
- Evaluation of Retrieval-Augmented Generation (RAG) systems under noisy, adversarial, and low-credibility retrieval conditions  
- Graph-based trust and reputation modeling for source-aware generation  

In parallel, I study **privacy leakage in virtual reality (VR) systems**, demonstrating how encrypted network traffic alone can reveal sensitive user activities and behaviors. This work highlights real-world risks at the intersection of AI, networking, and human-centered systems.

My research has been accepted at IEEE venues, and I actively publish, review, and build reproducible evaluation pipelines. I am particularly interested in **research internships** where rigorous experimentation, system-level thinking, and real-world impact matterâ€”especially in large-scale AI systems.

**Research interests**:
- LLM security, evaluation, and robustness  
- Retrieval-Augmented Generation (RAG) benchmarking  
- Adversarial testing and AI safety  
- Privacy-preserving ML systems  
- Network traffic analysis for VR/AR systems  

I am always happy to discuss research, collaborations, or internship opportunities.
